{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5cc6eb7-4087-4cc8-ac38-3e544709da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import pairwise_distances, normalized_mutual_info_score, f1_score\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy.stats import mode\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e6421f-6d03-4026-adbd-9ca5b5e239a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "(13233, 512)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "model = InceptionResnetV1(pretrained='casia-webface').eval().to(device)\n",
    "\n",
    "lfw_dir = 'lfw-deepfunneled' \n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "batch_size = 32 \n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "img_data = []\n",
    "label_data = []\n",
    "\n",
    "for person_name in os.listdir(lfw_dir):\n",
    "    person_dir = os.path.join(lfw_dir, person_name)\n",
    "    if os.path.isdir(person_dir):\n",
    "        for img_name in os.listdir(person_dir):\n",
    "            img_path = os.path.join(person_dir, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_data.append(preprocess(img))\n",
    "            label_data.append(person_name)\n",
    "\n",
    "            # Process in batches\n",
    "            if len(img_data) == batch_size:\n",
    "                batch = torch.stack(img_data).to(device) \n",
    "                with torch.no_grad():\n",
    "                    batch_embeddings = model(batch)\n",
    "                embeddings.extend(batch_embeddings.cpu().numpy())\n",
    "                labels.extend(label_data)\n",
    "                img_data, label_data = [], []\n",
    "\n",
    "if img_data:\n",
    "    batch = torch.stack(img_data).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch)\n",
    "    embeddings.extend(batch_embeddings.cpu().numpy())\n",
    "    labels.extend(label_data)\n",
    "\n",
    "features = np.array(embeddings)\n",
    "print(features.shape)\n",
    "np.save(\"features.npy\", features)\n",
    "np.save(\"labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fc91b1e-3567-4b78-b24f-d8853f9ff559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13233, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training One-Class SVM Models: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 13233/13233 [00:47<00:00, 281.20it/s]\n"
     ]
    }
   ],
   "source": [
    "features = np.load(\"features.npy\")\n",
    "print(features.shape)\n",
    "\n",
    "# Step 2.1: Compute the distance matrix\n",
    "distance_matrix = pairwise_distances(features, metric='cosine')\n",
    "epsilon = 0.23\n",
    "\n",
    "# Step 2.2: Construct neighborhoods\n",
    "neighborhoods = [np.where(distance_matrix[i] <= epsilon)[0] for i in range(len(features))]\n",
    "\n",
    "# Step 2.3: Train One-Class SVM models\n",
    "svdd_models = []\n",
    "gamma = 1.0 / features.shape[1]  # gamma as given in LIBSVM\n",
    "\n",
    "for i, neighbors in enumerate(tqdm(neighborhoods, desc=\"Training One-Class SVM Models\")):\n",
    "    if len(neighbors) < 1:\n",
    "        svdd_models.append(None)\n",
    "        continue\n",
    "\n",
    "    svm_features = features[neighbors]\n",
    "    \n",
    "    model = OneClassSVM(kernel='rbf', gamma=gamma, nu=0.5)  # nu as given in LIBSVM\n",
    "    model.fit(svm_features)\n",
    "    \n",
    "    svdd_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da0dbbd1-f94a-48af-8689-e1c4df1045b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Computing Decision Values: 100%|████████████████████████████████████████████████████████████████████████████████████████████| 13233/13233 [36:49<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU arrays\n",
    "features_gpu = cp.array(features)\n",
    "similarity_matrix_gpu = cp.zeros((len(features), len(features)))\n",
    "\n",
    "# Precompute all decision values for each feature neighborhood to minimize calls\n",
    "decision_values_gpu = cp.zeros((len(features), len(features)))\n",
    "\n",
    "# Convert features batch to CPU for each SVDD model in batches\n",
    "for i in tqdm(range(len(features)), desc=\"Batch Computing Decision Values\"):\n",
    "    feature_batch_cpu = cp.asnumpy(features_gpu)  # Transfer all features to CPU at once for batching\n",
    "    decision_values = svdd_models[i].decision_function(feature_batch_cpu)  # Run decision function on batch\n",
    "    decision_values_gpu[i, :] = cp.array(decision_values)  # Store back in GPU array\n",
    "\n",
    "cp.save('decision_values_gpu.npy', decision_values_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba101e7c-51f3-45c4-ae8a-5753cecf9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Pairwise Similarities: 100%|██████████████████████████████████████████████████████████████████████████████████| 87562761/87562761 [2:51:00<00:00, 8533.83it/s]\n"
     ]
    }
   ],
   "source": [
    "decision_values_gpu = cp.load('decision_values_gpu.npy')\n",
    "features_gpu = cp.array(features)\n",
    "\n",
    "similarity_matrix_gpu = cp.zeros((len(features), len(features)))\n",
    "weighted_similarity_matrix_gpu = cp.zeros((len(features), len(features)))\n",
    "epsilon = 1e-5 \n",
    "\n",
    "# Step 1: Calculate maximum neighborhood size and precompute neighborhood centers\n",
    "neighborhood_lengths = [len(n) for n in neighborhoods]\n",
    "max_neighborhood_size = max(neighborhood_lengths)\n",
    "\n",
    "# Create Neighborhood Index Matrix as a NumPy Array First\n",
    "neighborhood_indices_np = np.full((len(features), max_neighborhood_size), -1, dtype=np.int32)\n",
    "\n",
    "# Fill the neighborhood indices matrix\n",
    "for i, neighborhood in enumerate(neighborhoods):\n",
    "    neighborhood_indices_np[i, :len(neighborhood)] = np.array(neighborhood, dtype=np.int32)\n",
    "\n",
    "# Convert the filled NumPy array to a CuPy array\n",
    "neighborhood_indices = cp.array(neighborhood_indices_np)\n",
    "\n",
    "# Precompute centers for all neighborhoods and store them in a single matrix\n",
    "centers_gpu = cp.zeros((len(features), features_gpu.shape[1]))\n",
    "for i in range(len(features)):\n",
    "    neighbors_i = neighborhood_indices[i, neighborhood_indices[i] >= 0]\n",
    "    centers_gpu[i] = cp.mean(features_gpu[neighbors_i], axis=0)\n",
    "\n",
    "# Step 3: Compute Similarity Matrices in Smaller Batches with Sub-batching\n",
    "batch_size = 500  \n",
    "sub_batch_size = 100\n",
    "\n",
    "total_pairs = len(features) * (len(features) + 1) // 2\n",
    "with tqdm(total=total_pairs, desc=\"Computing Pairwise Similarities\") as pbar:\n",
    "    for i in range(len(features)):\n",
    "        neighbors_i = neighborhood_indices[i, neighborhood_indices[i] >= 0]\n",
    "        center_i = centers_gpu[i]\n",
    "\n",
    "        for j_start in range(i, len(features), batch_size):\n",
    "            j_end = min(j_start + batch_size, len(features))\n",
    "\n",
    "            for j_sub_start in range(j_start, j_end, sub_batch_size):\n",
    "                j_sub_end = min(j_sub_start + sub_batch_size, j_end)\n",
    "\n",
    "                neighbors_j_indices_sub = neighborhood_indices[j_sub_start:j_sub_end]\n",
    "                centers_j_sub = centers_gpu[j_sub_start:j_sub_end]\n",
    "\n",
    "                distances_ij_sub = cp.linalg.norm(features_gpu[neighbors_j_indices_sub] - center_i, axis=2) + epsilon\n",
    "                weights_ij_sub = 1 / distances_ij_sub\n",
    "\n",
    "                distances_ji_sub = cp.linalg.norm(features_gpu[neighbors_i] - centers_j_sub[:, None, :], axis=2) + epsilon\n",
    "                weights_ji_sub = 1 / distances_ji_sub\n",
    "\n",
    "                avg_decision_ij_sub = cp.mean(decision_values_gpu[i, neighbors_j_indices_sub], axis=1)  # Unweighted average\n",
    "                avg_decision_ji_sub = cp.mean(decision_values_gpu[j_sub_start:j_sub_end, neighbors_i], axis=1)  # Unweighted average\n",
    "\n",
    "                weighted_avg_decision_ij_sub = cp.mean(decision_values_gpu[i, neighbors_j_indices_sub] * weights_ij_sub, axis=1)  # Weighted average\n",
    "                weighted_avg_decision_ji_sub = cp.mean(decision_values_gpu[j_sub_start:j_sub_end, neighbors_i] * weights_ji_sub, axis=1)  # Weighted average\n",
    "\n",
    "                similarity_matrix_gpu[i, j_sub_start:j_sub_end] = similarity_matrix_gpu[j_sub_start:j_sub_end, i] = 0.5 * (avg_decision_ij_sub + avg_decision_ji_sub)  # Unweighted\n",
    "                weighted_similarity_matrix_gpu[i, j_sub_start:j_sub_end] = weighted_similarity_matrix_gpu[j_sub_start:j_sub_end, i] = 0.5 * (weighted_avg_decision_ij_sub + weighted_avg_decision_ji_sub)  # Weighted\n",
    "\n",
    "                pbar.update(j_sub_end - j_sub_start)\n",
    "                \n",
    "# Save both similarity matrices in GPU\n",
    "cp.save(\"similarity_matrix.npy\", similarity_matrix_gpu)\n",
    "cp.save(\"weighted_similarity_matrix.npy\", weighted_similarity_matrix_gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2ed9239-2744-439f-a738-74a39fd880fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.save(\"decision_values_gpu\",decision_values_gpu)\n",
    "similarity_matrix_cpu = similarity_matrix_gpu.get()  # Move to CPU\n",
    "weighted_similarity_matrix_cpu = weighted_similarity_matrix_gpu.get()  # Move to CPU\n",
    "\n",
    "np.save(\"similarity_matrix_cpu.npy\", similarity_matrix_cpu)\n",
    "np.save(\"weighted_similarity_matrix_cpu.npy\", weighted_similarity_matrix_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "413f3b62-82be-47e5-b32e-9fb3753a294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering done\n"
     ]
    }
   ],
   "source": [
    "# Load True labels\n",
    "labels = np.load(\"labels.npy\")\n",
    "similarity_matrix = np.load(\"similarity_matrix_cpu.npy\")\n",
    "k = 5  # Number of neighbors as per the algorithm\n",
    "\n",
    "# Step 1: Build M1 and D matrices\n",
    "def build_neighborhood_matrices(similarity_matrix, k):\n",
    "    N = similarity_matrix.shape[0]\n",
    "    M1 = np.zeros((N, k+1), dtype=int)\n",
    "    D = np.zeros((N, k+1))\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Get indices of k nearest neighbors based on similarity or distance\n",
    "        nearest_indices = np.argsort(-similarity_matrix[i])[:k+1]\n",
    "        M1[i] = nearest_indices\n",
    "        D[i] = similarity_matrix[i, nearest_indices]\n",
    "        \n",
    "    return M1, D\n",
    "\n",
    "M1, D = build_neighborhood_matrices(similarity_matrix, k)\n",
    "\n",
    "# Step 2: Construct M2 matrix with Mutual Neighborhood Values\n",
    "def construct_M2(M1, k):\n",
    "    N = M1.shape[0]\n",
    "    M2 = np.full((N, k+1), 100)  # Fill with arbitrary large value\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(1, k+1):\n",
    "            neighbor = M1[i, j]\n",
    "            if i in M1[neighbor, 1:k+1]:\n",
    "                M2[i, j] = j  # Assign MNV based on neighborhood depth\n",
    "            \n",
    "    return M2\n",
    "\n",
    "M2 = construct_M2(M1, k)\n",
    "\n",
    "def perform_clustering(M1, D, M2):\n",
    "    N= 13233;\n",
    "    labels = np.arange(N);\n",
    "    clusters = {i: [i] for i in range(N)}  # Initialize each point in its own cluster\n",
    "    current_clusters = N  # Start with each sample as its own cluster\n",
    "\n",
    "    # Iterate over MNV values, starting from 2 up to the maximum (2 * k)\n",
    "    for mnv in range(2, 2*k + 1):\n",
    "        pairs_to_merge = []\n",
    "        \n",
    "        # Collect all pairs with the current MNV value\n",
    "        for i in range(N):\n",
    "            for j in range(1, k+1):\n",
    "                if M2[i, j] == mnv:\n",
    "                    neighbor = M1[i, j]\n",
    "                    dist = D[i, j]\n",
    "                    pairs_to_merge.append((dist, i, neighbor))\n",
    "        \n",
    "        # Sort pairs by distance to prioritize merging closest pairs first\n",
    "        pairs_to_merge.sort()\n",
    "\n",
    "        # Merge pairs while ensuring each pair is only merged if in different clusters\n",
    "        for dist, i, neighbor in pairs_to_merge:\n",
    "            if labels[i] != labels[neighbor]:  # Only merge if they are in different clusters\n",
    "                new_label = min(labels[i], labels[neighbor])\n",
    "                old_label = max(labels[i], labels[neighbor])\n",
    "\n",
    "                # Check if old_label is still in clusters before proceeding\n",
    "                if old_label in clusters:\n",
    "                    labels[labels == old_label] = new_label  # Update labels to reflect the merge\n",
    "                    clusters[new_label].extend(clusters[old_label])\n",
    "                    del clusters[old_label]  # Safely delete the old cluster entry\n",
    "                    current_clusters -= 1  # Reduce cluster count after each merge\n",
    "                \n",
    "        # After each MNV level, check if clusters are naturally separated\n",
    "        if current_clusters == 1:\n",
    "            print(\"All data points merged into a single cluster.\")\n",
    "            break\n",
    "\n",
    "    return labels, clusters\n",
    "\n",
    "# Apply the clustering function without a target cluster count\n",
    "final_labels, final_clusters = perform_clustering(M1, D, M2)\n",
    "\n",
    "print(\"Clustering done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ae9b86-60c0-491e-afed-60442aed127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_labels = np.load(\"labels.npy\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(ground_truth_labels)\n",
    "mapped_clusters = np.zeros_like(encoded_labels)\n",
    "\n",
    "for cluster_id, indices in final_clusters.items():\n",
    "    cluster_indices = np.array(indices)\n",
    "    most_common_label = mode(encoded_labels[cluster_indices])[0][0]\n",
    "    mapped_clusters[cluster_indices] = most_common_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3bc11a-1e2e-4147-911a-a5b78ef3c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bcubed_fmeasure(final_clusters, encoded_labels):    \n",
    "    n_samples = len(encoded_labels)\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    # Iterate over each sample\n",
    "    for i in range(n_samples):\n",
    "        true_class = encoded_labels[i]\n",
    "\n",
    "        for cluster_id, indices in final_clusters.items():\n",
    "            if i in indices:\n",
    "                predicted_cluster = cluster_id\n",
    "                break\n",
    "\n",
    "        cluster_indices = np.array(final_clusters[predicted_cluster])\n",
    "        precision = np.sum(encoded_labels[cluster_indices] == true_class) / len(cluster_indices)\n",
    "\n",
    "        class_indices = np.where(encoded_labels == true_class)[0]\n",
    "        recall = np.sum(encoded_labels[cluster_indices] == true_class) / len(class_indices)\n",
    "\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0  # Avoid division by zero\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "\n",
    "    bcubed_f1 = np.mean(precision_list)\n",
    "    return bcubed_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e24d785-0c62-42c9-a4c5-c728914dc54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-measure: 0.9252946252709756\n",
      "NMI: 0.8895425126519361\n",
      "B Cubed F-measure: 0.9267812260306397\n"
     ]
    }
   ],
   "source": [
    "f_measure = f1_score(encoded_labels, mapped_clusters, average='weighted')\n",
    "nmi_score = normalized_mutual_info_score(final_labels, ground_truth_labels)\n",
    "b_cubed_f_measure = bcubed_fmeasure(final_clusters, ground_truth_labels)\n",
    "\n",
    "print(f\"F1-measure: {f_measure}\")\n",
    "print(f\"NMI: {nmi_score}\")\n",
    "print(f\"B Cubed F-measure: {b_cubed_f_measure}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead96c6-fde0-4b27-894d-c4171853567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out LIBSVM by locally downloading it (NOT WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59aa1d-7aa0-4387-bfaa-82991949a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your features\n",
    "features = np.load(\"features.npy\")\n",
    "print(features.shape)\n",
    "\n",
    "# Step 2.1: Compute the distance matrix\n",
    "distance_matrix = pairwise_distances(features, metric='cosine')\n",
    "epsilon = 0.23\n",
    "\n",
    "# Step 2.2: Construct neighborhoods\n",
    "neighborhoods = [np.where(distance_matrix[i] <= epsilon)[0] for i in range(len(features))]\n",
    "\n",
    "# Step 2.3: Train SVDD models using the compiled LIBSVM\n",
    "svdd_models = []\n",
    "\n",
    "# Define paths to the compiled LIBSVM executables\n",
    "svm_train_path = '/home/ipr_students/Divij/libsvm-3.22/svm-train'\n",
    "\n",
    "for i, neighbors in enumerate(tqdm(neighborhoods, desc=\"Training SVDD Models\")):\n",
    "    if len(neighbors) < 1:\n",
    "        svdd_models.append(None)\n",
    "        continue\n",
    "    \n",
    "    # Get features for the neighborhood\n",
    "    svm_features = features[neighbors]\n",
    "    labels = [1] * len(svm_features)\n",
    "    \n",
    "    # Write features to a temporary training file in LIBSVM format\n",
    "    train_file = 'training_data.txt'\n",
    "    with open(train_file, 'w') as f:\n",
    "        for label, feature in zip(labels, svm_features):\n",
    "            # Create a line in LIBSVM format\n",
    "            line = f\"{label} \" + \" \".join(f\"{j + 1}:{val}\" for j, val in enumerate(feature, start=1))\n",
    "            f.write(line + \"\\n\")  # Ensure a newline at the end of each line\n",
    "\n",
    "    params = f'-s 5'\n",
    "\n",
    "    # Train the SVDD model using the LIBSVM executable\n",
    "    model_file = 'model_file'\n",
    "    subprocess.run([svm_train_path, params, train_file, model_file])\n",
    "\n",
    "    # Store the model file name for reference\n",
    "    svdd_models.append(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c0d9b-3e76-4770-96ae-2b9f610bb978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARLIER VERSION (PHASE 1) : THIS CODE IS NOT OPTIMISED FOR GPU : CALCULATING SIMILARITY MATRICES\n",
    "similarity_matrix = np.zeros((len(features), len(features)))\n",
    "similarity_matrix2 = np.zeros((len(features), len(features)))\n",
    "epsilon = 1e-5  # Small constant to avoid division by zero\n",
    "\n",
    "for i in tqdm(range(len(features)), desc=\"Computing Similarities\"):\n",
    "    for j in range(i, len(features)):  # Symmetric matrix, only compute once\n",
    "        # Initialize variables for the sums\n",
    "        sum_decision_ij = 0\n",
    "        sum_decision_ji = 0\n",
    "        weighted_sum_decision_ij = 0\n",
    "        weighted_sum_decision_ji = 0\n",
    "        # Compute sums for neighbors of j in i's neighborhood\n",
    "        for z in neighborhoods[j]:\n",
    "            decision_value_i = svdd_models[i].decision_function(features[z].reshape(1, -1))[0]\n",
    "            sum_decision_ij += decision_value_i\n",
    "            # Calculate and add weighted decision function\n",
    "            weight_i = 1 / (np.linalg.norm(features[z] - features[i]) + epsilon)\n",
    "            weighted_sum_decision_ij += weight_i * decision_value_i\n",
    "\n",
    "        # Compute sums for neighbors of i in j's neighborhood\n",
    "        for z in neighborhoods[i]:\n",
    "            decision_value_j = svdd_models[j].decision_function(features[z].reshape(1, -1))[0]\n",
    "            sum_decision_ji += decision_value_j\n",
    "            \n",
    "            # Calculate and add weighted decision function\n",
    "            weight_j = 1 / (np.linalg.norm(features[z] - features[j]) + epsilon)\n",
    "            weighted_sum_decision_ji += weight_j * decision_value_j\n",
    "\n",
    "        # Calculate average decision values\n",
    "        if len(neighborhoods[j]) > 0:\n",
    "            avg_decision_ij = sum_decision_ij / len(neighborhoods[j])\n",
    "            avg_weighted_decision_ij = weighted_sum_decision_ij / len(neighborhoods[j])\n",
    "        else:\n",
    "            avg_decision_ij = 0\n",
    "            avg_weighted_decision_ij = 0\n",
    "\n",
    "        if len(neighborhoods[i]) > 0:\n",
    "            avg_decision_ji = sum_decision_ji / len(neighborhoods[i])\n",
    "            avg_weighted_decision_ji = weighted_sum_decision_ji / len(neighborhoods[i])\n",
    "        else:\n",
    "            avg_decision_ji = 0\n",
    "            avg_weighted_decision_ji = 0\n",
    "        # Calculate similarity and weighted similarity\n",
    "        similarity_matrix[i, j] = similarity_matrix[j, i] = 0.5 * (avg_decision_ij + avg_decision_ji)\n",
    "        similarity_matrix2[i, j] = similarity_matrix2[j, i] = 0.5 * (avg_weighted_decision_ij + avg_weighted_decision_ji)\n",
    "\n",
    "np.save(\"similarity_matrix.npy\", similarity_matrix)\n",
    "np.save(\"similarity_matrix2.npy\", similarity_matrix2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
